# Overfitting 발생시 해결 방법
 - 학습된 모델이 Train Data에만 최적화 된 상태
 - 과적합 발생시 validation 의 loss가 줄다가 다시 상승곡선을 그리게 된다.

  <b>1) Train Data를 추가 해준다.</b>
  - 추가로 데이터를 수집해야해서 현실적으로 어려움

  <b>2) 성능을 낮추지 않는 선에서 Model의 Capacity를 낮게 조절해준다.</b>
      
- 파라미터 개수 : 모델의 파라미터의 개수를 줄임 <br>
        방법 1. 노드의 개수를 줄이기 <br>
        방법 2. 히든레이어 개수를 제거 <br>
        방법 3. 인풋사이즈를 줄임 (제일 안좋은 선택 _ 성능이 안좋아짐) <br>
-   L2 Regularization (L2규제 , Lasso)
    - 오차최소화를 방해하는 방법 
    - 가중치의 제곱에 비레하는 Noise를 Cost Function (MSE or CEE) 에 추가 (가중치 감쇠)
    - Noise : 𝒘 가 + 일수도 있고 - 일수도 있으니 제곱해서 더해줌( 0~ 1사이의 값인 𝜶 를 곱해서 일정비율로 적용시킴) 
    - 규제를 가지면서도, 오차를 최소화시키려면 가능한 𝒘가 작아야함 (결국 곱해지는 𝜶 규제의 크기가 작아져야 하니까)
    - 결과적으로 𝒘 효과를 낮춰주는 역할을 함
    - L2 Regularization : 𝜶 * ∑𝒘² <br>
      L1 Regularization(L1규제,릿지) : 𝜶 * ∑|𝒘| 
    - 다항회귀의 경우 과적합 가능성이 높은데, 규제 선형모델을 통해 regulization하면 𝒘가 작아지면서 차원이 축소되는 효과를 가져온다.  <br>
    (가중치가 0 에 가까워지면 해당 항의 영향도가 낮아지는거니까)
    - 릿지와 라쏘는 각각 독립적으로 적용 가능하며 동시에 적용시 엘라스틱넷이라고 부른다.


     -  Dropout
          - 학습과정에서 네트워크의 일부 연결을 무작위로 제외시킴
          - Model.add (laters.Dropout(0.4)) 
          - 괄호 안의 숫자는 랜덤으로 끊어지는 네트워크 비중의미 (40%)
     
    

<b> 3) Batch normalization [statistics] </b>
 - 성능개선을 위한 방안이지만, side_effect로 오버피팅을 줄이는 효과도 발생됨.

 - 데이터/ capacity와 상관없지만 성능이 좋다
 -  X data : 정규분포 Normal Distribution
 - 학습 때마다 분포가 바뀜
 - activation 활성화 함수 전에 정규화를 실행하여 정규화된 입력값을 준다.
 - FC > BN > Relu > FC > BN > Relu
 - 정규분포화 하기위해 평균과 분산을 어떻게 잡으면 좋을지 잡기 위해 parameter가 존재한다.

```위의 방법들을 조합해서 최적의 결과를 찾아보는 것이 중요하고 parameter 개수를 줄이는 것은 가장 마지막에 고려 해보는 것이 좋음```


